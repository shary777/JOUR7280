{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JOUR7280/COMM7780 Big Data Analytics for Media and Communication\n",
    "# Tutorial: Scraping multiple pages\n",
    "In this tutorial we’ll learn to scrape multiple web pages with Python using `BeautifulSoup` and `requests`. We will be scraping hundreds of movies from [imdb](https://www.imdb.com/?ref_=nv_home) and store data using the `Pandas` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from time import sleep\n",
    "from time import time\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping single page\n",
    "If you go on IMDB’s advanced search [page](https://www.imdb.com/search/), you can browse movies by year:\n",
    "<img src=\"../figs/home.png\" alt=\"drawing\" width=\"550\"/>\n",
    "<img src=\"../figs/search.png\" alt=\"drawing\" width=\"550\"/>\n",
    "<img src=\"../figs/search-year.png\" alt=\"drawing\" width=\"550\"/>\n",
    "Let’s choose \"feature film\", browse by year 2020, sort the movies on the number of votes, we'll arrive [this page](https://www.imdb.com/search/title/?title_type=feature&release_date=2020-01-01,2020-12-31&sort=num_votes,desc)  \n",
    "- A feature film, feature-length film, or theatrical film is a film with a running time long enough to be considered the principal or sole film to fill a program. (wiki)\n",
    "\n",
    "### <span style=\"color:DARKTURQUOISE\">Note</span>  \n",
    "As a side note, if you run the code from a country where English is not the main language, it’s very likely that you’ll get some of the movie names translated into the main language of that country.\n",
    "\n",
    "Most likely, this happens because the server infers your location from your IP address. Even if you are located in a country where English is the main language, you may still get translated content. This may happen if you’re using a VPN while you’re making the `GET` requests.\n",
    "\n",
    "If you run into this issue, pass the following values to the `headers` parameter of the `get()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\"Accept-Language\": \"en-US, en;q=0.5\"}\n",
    "url = 'https://www.imdb.com/search/title/?title_type=feature&release_date=2020-01-01,2020-12-31&sort=num_votes,desc'\n",
    "page = requests.get(url, headers = headers)\n",
    "print(page.status_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will communicate the server something like “I want the linguistic content in American English (en-US). If en-US is not available, then other types of English (en) would be fine too (but not as much as en-US).”. The `q` parameter indicates the degree to which we prefer a certain language. If not specified, then the values is set to `1` by default, like in the case of en-US. You can read more about this [here](https://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.4).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract movie information\n",
    "<img src=\"../figs/page1.png\" alt=\"drawing\" width=\"550\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "html_soup = BeautifulSoup(page.content, 'html.parser')\n",
    "movie_containers = html_soup.find_all('div', class_ = 'lister-item mode-advanced')\n",
    "print(len(movie_containers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we’ll extract, by turn, each item of interest for each container:\n",
    "\n",
    "- The name of the movie.\n",
    "- The year of release.\n",
    "- The IMDB rating.\n",
    "- The number of votes.\n",
    "\n",
    "### The number of votes\n",
    "The number of votes is contained within a `<span>` tag. Its distinctive mark is a `name` attribute with the value `nv`.\n",
    "<img src=\"../figs/vote.png\" alt=\"drawing\" width=\"500\"/>\n",
    "\n",
    "The `name` attribute is different from the `class` attribute. Using BeautifulSoup we can access elements by any attribute. The `find()` and `find_all()` functions have a parameter named `attrs`. To this we can pass in the attributes and values we are searching for as a dictionary. You can read more about this [here](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#the-keyword-arguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "container = movie_containers[0]\n",
    "vote = container.find('span', attrs={'name':'nv'})['data-value']\n",
    "vote"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could use `.get_text()` notation to access the `<span>` tag’s content. It would be better though if we accessed the value of the data-value attribute. This way we can convert the extracted datapoint to an `int` without having to strip a comma.\n",
    "\n",
    "You can treat a Tag object just like a `dictionary`. The HTML attributes are the dictionary’s keys. The values of the HTML attributes are the values of the dictionary’s keys. This is how we can access the value of the `data-value` attribute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using of CSS Selector\n",
    "\n",
    "We extract the movie title by using CSS selectors [(more about CSS selectors)](https://www.htmldog.com/guides/css/intermediate/classid/).\n",
    "The `select` method allows you to **find tags beneath other tags**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = container.select('.lister-item-header a')[0].get_text()\n",
    "print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine everything\n",
    "In the next code block we:\n",
    "\n",
    "- Declare some list variables to have something to store the extracted data in.\n",
    "- Loop through each container in movie_containers (the variable which contains all the 50 movie containers).\n",
    "- Extract the information of each moive container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "headers = {\"Accept-Language\": \"en-US, en;q=0.5\"}\n",
    "url = 'https://www.imdb.com/search/title/?title_type=feature&release_date=2020-01-01,2020-12-31&sort=num_votes,desc'\n",
    "page = requests.get(url, headers = headers)\n",
    "print(page.status_code)\n",
    "\n",
    "html_soup = BeautifulSoup(page.content, 'html.parser')\n",
    "movie_containers = html_soup.find_all('div', class_ = 'lister-item mode-advanced')\n",
    "\n",
    "# Lists to store the scraped data in\n",
    "names = []\n",
    "years = []\n",
    "imdb_ratings = []\n",
    "votes = []\n",
    "\n",
    "\n",
    "# Extract data from individual movie container\n",
    "for container in movie_containers:\n",
    "    # The name\n",
    "    name = container.select('.lister-item-header a')[0].get_text()\n",
    "    names.append(name)\n",
    "    # The year\n",
    "    year = container.find('span', class_ = 'lister-item-year').get_text()\n",
    "    years.append(year)\n",
    "    # The IMDB rating\n",
    "    imdb = float(container.find('strong').get_text())\n",
    "    imdb_ratings.append(imdb)\n",
    "    # The number of votes\n",
    "    vote = container.find('span', attrs={'name':'nv'})['data-value']\n",
    "    votes.append(int(vote))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's store the collected data into a `dataframe`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_df = pd.DataFrame({'movie': names,\n",
    "'year': years,\n",
    "'rating': imdb_ratings,\n",
    "'votes': votes\n",
    "})\n",
    "print(test_df.info())\n",
    "test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping multiple pages\n",
    "Scraping multiple pages is a bit more challenging. We’ll build upon our one-page script by doing more things:\n",
    "\n",
    "1. Making all the requests we want from within the loop.\n",
    "1. Controlling the loop’s rate to avoid bombarding the server with requests.\n",
    "\n",
    "We’ll scrape the first 2 pages of year 2020. Each page has 50 movies, so we’ll scrape data for 100 movies. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying the URL structure\n",
    "Our challenge now is to make sure we understand the logic of the URL as the pages we want to scrape change. If we can’t understand this logic enough so we can implement it into code, then we’ll reach a dead end.  \n",
    "Let’s browse by year 2020, sort the movies by number of votes, then switch to the next page. We’ll arrive at this [web page](https://www.imdb.com/search/title/?title_type=feature&release_date=2020-01-01,2020-12-31&sort=num_votes,desc&start=51&ref_=adv_nxt), which has this URL:  \n",
    "\n",
    "https://www.imdb.com/search/title/?title_type=feature&release_date=2020-01-01,2020-12-31&sort=num_votes,desc&start=51&ref_=adv_nxt  \n",
    "\n",
    "\n",
    "In the URL above, you can see that the URL has several parameters after the question mark:\n",
    "\n",
    "- release_date — Shows only the movies released in a specific year.\n",
    "- sort — Sorts the movies on the page. sort=num_votes,desc translates to sort by number of votes in a descending order.\n",
    "- start — Specifies the number of first movie item in current page.\n",
    "- ref_ — Takes us to the the next or the previous page. The reference is the page we are currently on. adv_nxt and adv_prv are two possible values. They translate to advance to the next page, and advance to the previous page, respectively.\n",
    "\n",
    "If you change `start` value to 1, the URL become:\\\n",
    "https://www.imdb.com/search/title/?title_type=feature&release_date=2020-01-01,2020-12-31&sort=num_votes,desc&start=1&ref_=adv_nxt which will navigate to the first page.\n",
    "\n",
    "If you navigate through those pages and observe the URL, you will notice that only the values of the parameters change. This means we can write a script to match the logic of the changes and make far fewer requests to scrape our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing the URL’s parameters\n",
    "As we are making the requests, we’ll only have to vary the values of `start` parameters of the URL. Let’s prepare the values we’ll need for the forthcoming loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_nums = []\n",
    "for i in range(0, 5):\n",
    "    print(i)\n",
    "    start_nums.append(str(i*50+1))\n",
    "start_nums"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Controlling the crawl-rate\n",
    "Controlling the rate of crawling is beneficial for us, and for the website we are scraping. If we avoid hammering the server with tens of requests per second, then we are much less likely to get our IP address banned. We also avoid disrupting the activity of the website we scrape by allowing the server to respond to other users’ requests too.\n",
    "\n",
    "We’ll control the loop’s rate by using the `sleep()` function from Python’s `time` module. `sleep()` will **pause** the execution of the loop for a specified amount of seconds.\n",
    "\n",
    "To mimic human behavior, we’ll vary the amount of waiting time between requests by using the `randint()` function from the Python’s random module. `randint()` randomly generates integers within a specified interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "from time import time\n",
    "from random import randint\n",
    "\n",
    "# Pause the loop\n",
    "sleep(randint(3,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Piecing everything together\n",
    "Now let’s piece together everything we’ve done so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# headers = {\"Accept-Language\": \"en-US, en;q=0.5\"}\n",
    "\n",
    "# Redeclaring the lists to store data in\n",
    "names = []\n",
    "years = []\n",
    "imdb_ratings = []\n",
    "votes = []\n",
    "\n",
    "start_nums = []\n",
    "for i in range(0, 2):\n",
    "    start_nums.append(str(i*50+1))\n",
    "\n",
    "# For every page in the interval\n",
    "for start_num in start_nums:\n",
    "\n",
    "    # Make a get request\n",
    "    url = 'https://www.imdb.com/search/title/?title_type=feature&release_date=2020-01-01,2020-12-31&sort=num_votes,desc&start='\\\n",
    "        + start_num + '&ref_=adv_nxt'\n",
    "    print(url)\n",
    "    response = requests.get(url, headers = headers)\n",
    "\n",
    "    # Pause the loop\n",
    "    sleep(randint(3,6))\n",
    "\n",
    "       \n",
    "    # Parse the content of the request with BeautifulSoup\n",
    "    html_soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Select all the 50 movie containers from a single page\n",
    "    movie_containers = html_soup.find_all('div', class_ = 'lister-item mode-advanced')\n",
    "\n",
    "    # For every movie of these 50\n",
    "    # Extract data from individual movie container\n",
    "    for container in movie_containers:\n",
    "        # The name\n",
    "        name = container.select('.lister-item-header a')[0].get_text()\n",
    "        names.append(name)\n",
    "        # The year\n",
    "        year = container.find('span', class_ = 'lister-item-year').get_text()\n",
    "        years.append(year)\n",
    "        # The IMDB rating\n",
    "        imdb = float(container.find('strong').get_text())\n",
    "        imdb_ratings.append(imdb)\n",
    "        # The number of votes\n",
    "        vote = container.find('span', attrs={'name':'nv'})['data-value']\n",
    "        votes.append(int(vote))\n",
    "\n",
    "\n",
    "movie_ratings = pd.DataFrame({'movie': names,\n",
    "'year': years,\n",
    "'imdb': imdb_ratings,\n",
    "'votes': votes\n",
    "})\n",
    "print(movie_ratings.info())\n",
    "movie_ratings.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping single movie pages\n",
    "Next, we'll explore the single move pages to extract the writers information of each movie.\n",
    "### Find the link to the movie\n",
    "<img src=\"../figs/link.png\" alt=\"drawing\" width=\"600\"/>\n",
    "\n",
    "The link to the movie is \" https://www.imdb.com/title/tt6723592/?ref_=adv_li_tt \". Thus, we concatenate the string “https://www.imdb.com\" with “/title/tt6723592/?ref_=adv_li_tt” to get the URL of the first movie \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain = 'https://www.imdb.com'\n",
    "sub_url = container.select('.lister-item-header a')[0]['href']\n",
    "print('sub url:', sub_url)\n",
    "# url of movie page\n",
    "movie_url = domain + sub_url\n",
    "print('movie url:', movie_url)\n",
    "movie_html = requests.get(movie_url, headers = headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract information & store to dataframe\n",
    "Now, let's extract writers information from each single movie page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "headers = {\"Accept-Language\": \"en-US, en;q=0.5\"}\n",
    "url = 'https://www.imdb.com/search/title/?title_type=feature&release_date=2020-01-01,2020-12-31&sort=num_votes,desc&start=1&ref_=adv_nxt'\n",
    "page = requests.get(url, headers = headers)\n",
    "print(page.status_code)\n",
    "\n",
    "html_soup = BeautifulSoup(page.content, 'html.parser')\n",
    "movie_containers = html_soup.find_all('div', class_ = 'lister-item mode-advanced')\n",
    "\n",
    "\n",
    "writers = []\n",
    "domain = 'https://www.imdb.com'\n",
    "for container in movie_containers:\n",
    "    sub_url = container.select('.lister-item-header a')[0]['href']\n",
    "    # url of movie page\n",
    "    movie_url = domain + sub_url\n",
    "    movie_html = requests.get(movie_url, headers = headers)\n",
    "    # Pause the loop\n",
    "    sleep(randint(3,6))\n",
    "\n",
    "    # extract writers information\n",
    "    movie_soup = BeautifulSoup(movie_html.content, 'html.parser') # parser of individual movie page\n",
    "    summary = movie_soup.find(class_ = 'plot_summary')\n",
    "    names = summary.find_all(class_ = 'credit_summary_item')[1].find_all('a') \n",
    "    \n",
    "    writer = ''\n",
    "    for name in names:\n",
    "        writer = writer + name.get_text() + ' & '\n",
    "\n",
    "    print(writer[0:len(writer)-2])\n",
    "    writers.append(writer[0:len(writer)-2])\n",
    "\n",
    "writer_df = pd.DataFrame({'writer': writers})\n",
    "print(writer_df.info())\n",
    "writer_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenate dataframes\n",
    "Finally, let's combine the writers' information with previous collected information.\n",
    "\n",
    "We use `pd.concat()` function to achieve this.  \n",
    "`axis` : {0, 1, …}, default 0. The axis to concatenate along.\n",
    "- axis=0: concatenate on rows; \n",
    "- axis=1: concatenate on columns\n",
    "\n",
    "[more about concat on dataframe](https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result = pd.concat([test_df, writer_df], axis=1)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save data to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv('../data/imdb.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The codes in this notebook are modified from various sources. All codes are for educational purposes only and released under the CC1.0."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
